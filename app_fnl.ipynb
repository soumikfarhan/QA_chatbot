{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Query Tool for Biomedical Researchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "from langchain.vectorstores import FAISS\n",
    "import streamlit as st\n",
    "from langchain.chat_models import ChatOpenAI \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain import hub\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the open ai api key from the key.txt file\n",
    "\n",
    "# Path to your key text file\n",
    "key_file_path = 'key.txt'\n",
    "\n",
    "# Read the key from the text file\n",
    "with open(key_file_path, 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "# Set the API key as environment variable\n",
    "os.environ['OPENAI_API_KEY'] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved embeddings from the vector store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "new_vector_store = FAISS.load_local(\"vector_db\", embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setups for the rag systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the retriver to generate top results (taking top k = 1 results)\n",
    "retriever = new_vector_store.as_retriever(search_kwargs={\"k\": 1})\n",
    "# the prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# the llm\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatiing the docs to pass inside the chain for the context\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formulate the chain\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the question invoke for generating answers from llm\n",
    "def generate_answer(query):\n",
    "   ans = rag_chain_with_source.invoke(query)\n",
    "   return   ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "App interfacing using streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    st.header(\"A Query Tool for Biomedical Researchers\")\n",
    "    # ChatInput\n",
    "    prompt = st.chat_input(\"Enter your questions here\")\n",
    "\n",
    "   # Storing the prompts and responses history \n",
    "   \n",
    "    if \"user_prompt_history\" not in st.session_state:\n",
    "       st.session_state[\"user_prompt_history\"]=[]\n",
    "    if \"chat_answers_history\" not in st.session_state:\n",
    "       st.session_state[\"chat_answers_history\"]=[]\n",
    "       \n",
    "   # in case, you want to store the history and send it to the llm    \n",
    "   #  if \"chat_history\" not in st.session_state:\n",
    "   #     st.session_state[\"chat_history\"]=[]\n",
    "\n",
    "    if prompt:\n",
    "       with st.spinner(\"Generating......\"):\n",
    "\n",
    "            output=generate_answer(prompt)\n",
    "          # Seperating the metadata for reference\n",
    "            ref = output['context'][0].metadata\n",
    "            \n",
    "            answer = output['answer']\n",
    "            source = ref['source']\n",
    "            # display the answer with reference\n",
    "            st.session_state[\"chat_answers_history\"].append(f\"{answer}\\n\\nThe references:\\n[{source}](your_ref_here)\")\n",
    "            st.session_state[\"user_prompt_history\"].append(prompt)\n",
    "            \n",
    "            # in case the whole chat history can be send to the llm for conversational purposes\n",
    "            # st.session_state[\"chat_history\"].append((prompt,output['answer']))\n",
    "\n",
    "    # Displaying the chat history for a session\n",
    "\n",
    "    if st.session_state[\"chat_answers_history\"]:\n",
    "       for i, j in zip(st.session_state[\"chat_answers_history\"],st.session_state[\"user_prompt_history\"]):\n",
    "          message1 = st.chat_message(\"user\")\n",
    "          message1.write(j)\n",
    "          message2 = st.chat_message(\"assistant\")\n",
    "          message2.write(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
